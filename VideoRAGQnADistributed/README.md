# Video RAG Distributed
**Version for NEX NESS 2024 demo**

## Introduction

This is a distributed version of VideoRAGQnA with 4 micoserverices:

1. vector store
2. LLM inference
3. vid2text (video describe & ingest)
4. rag-agent

each is separated as independent microservices.

Video RAG is a framework that retrives video based on provided user prompt. It uses both video scene description generated by open source vision models (ex video-llama, video-llava etc.) as text embeddings and frames as image embeddings to perform vector similarity search. The provided solution also supports feature to retrieve more similar videos without prompting it. (see the example video below)


## Tools

- **UI**: TODO
- **Vector Storage**: Chroma DB **or** Intel's VDMS
- **Image Embeddings**: CLIP
- **Text Embeddings**: all-MiniLM-L12-v2
- **RAG Retriever**: Langchain Ensemble Retrieval
- **Model server**: Ray serve with Llama2-7b

## Prerequisites

There are 10 example videos present in ```vid2text/videos``` along with their description generated by open-source vision model.
If you want these video RAG to work on your own videos, make sure it matches below format.

## File Structure

```bash
vid2text/
.
├── scene_description
│   ├── op_10_0320241830.mp4.txt
│   ├── op_1_0320241830.mp4.txt
│   ├── op_19_0320241830.mp4.txt
│   ├── op_21_0320241830.mp4.txt
│   ├── op_24_0320241830.mp4.txt
│   ├── op_31_0320241830.mp4.txt
│   ├── op_47_0320241830.mp4.txt
│   ├── op_5_0320241915.mp4.txt
│   ├── op_DSCF2862_Rendered_001.mp4.txt
│   └── op_DSCF2864_Rendered_006.mp4.txt
└── videos
    ├── op_10_0320241830.mp4
    ├── op_1_0320241830.mp4
    ├── op_19_0320241830.mp4
    ├── op_21_0320241830.mp4
    ├── op_24_0320241830.mp4
    ├── op_31_0320241830.mp4
    ├── op_47_0320241830.mp4
    ├── op_5_0320241915.mp4
    ├── op_DSCF2862_Rendered_001.mp4
    └── op_DSCF2864_Rendered_006.mp4
```

## Setup and Installation

VideoRAGQnA supports both **docker compse setup** and **Helm chart** setup. For docker compose setup, please skip the `helm setup` steps. And for Helm usage, please skip the `docker compse setup` steps. 
> TODO: Helm chart setup

### Vector store microservice setup

Please build and run the service as the following steps:

```bash
cd VideoRAGQnADistributed/vectorDB_service
docker compose build
# docker compose setup:
docker compose up 
```

Currently ChromaDB and Intel's VDMS are supported.
For full API docs, please visit `http://server_ip:9001/docs` for automatic interactive API documentation

### Model server microservice setup

Please first setup your [huggingface hub API token](https://huggingface.co/login?next=%2Fsettings%2Ftokens).

And download the `llama-2-7b-chat-hf` model file as following. Please apply for extra permission on the model page:

```bash
export HUGGINGFACEHUB_API_TOKEN='<your HF token>'
cd VideoRAGQnADistributed/ckpt
git lfs install
git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
```

Please build and start the model server container as following:

```bash
cd VideoRAGQnADistributed/model_serve
docker compose build
# docker compose setup:
docker compose up
```

### vid2text (video ingest) & rag-agent setup

The vid2text aims to replace the video-ingest by adding the video description method and video upload method. The rag-agent is used to forward complex requests to target microservices.

The configs are listed in `VideoRAGQnADistributed/conf/config.yaml`. Please change the host IP of your microservices in the yaml file.

```yaml
vector_db:
  choice_of_db: 'chroma' # Supported databases [vdms, chroma]
  host: <change the IP here>
model_server:
  host: <change the IP here>
vid2text:
  host: <change the IP here>
```
Download the `Video-LLaMA-2-7b-Finetuned` model file as following.
```bash
export HUGGINGFACEHUB_API_TOKEN='<your HF token>'
cd VideoRAGQnADistributed/ckpt
git clone https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-2-7B-Finetuned
```

Build and start the microservices as following:

```bash
cd VideoRAGQnADistributed/
docker compose build
# docker compose setup:
docker compose up
```
